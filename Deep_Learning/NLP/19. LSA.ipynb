{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSA (Latent Semantic Analysis, LSA)\n",
    "- BoW에 기반한 DTM, TF-IDF는 단어 빈도 수를 이용한 수치화 방법. 따라서 단어의 의미를 고려하지 못함(=토픽을 고려하지 못함)\n",
    "- LSA는 잠재된 의미를 이끌어내는 방법. LSI(Latent Semantic Indexing)이라고도 불림\n",
    "- LSA에서는 특이값 분해(SVD)가 중요하다.\n",
    "  - A = UΣV(T)\n",
    "  - 여기서 각 행렬의 모든 요소를 선택하면 Full SVD가 되고, 상위값 t개만 남기게 되면 Truncated SVD가 된다. 이 t가 바로 찾고자 하는 토픽의 수를 반영한 하이퍼 파라미터 값이다. \n",
    "  - t를 크게 잡으면 기존 행렬에서 다양한 의미를 가져갈 수 있지만, t를 작게 잡아야만 노이즈를 제거할 수 있다.(설명력이 높은 정보만 남긴다.)\n",
    "\n",
    "- LSA는 DTM이나 TF-IDF 행렬에 Truncated SVD를 사용하여 차원을 축소시키고, 단어들의 잠재적인 의미를 끌\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Full SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DTM의 크기:  (4, 9)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "A = np.array([[0,0,0,1,0,1,1,0,0],[0,0,0,1,1,0,1,0,0],[0,1,1,0,2,0,0,0,0],[1,0,0,0,0,0,0,1,1]])\n",
    "print('DTM의 크기: ', np.shape(A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1, 0, 1, 1, 0, 0],\n",
       "       [0, 0, 0, 1, 1, 0, 1, 0, 0],\n",
       "       [0, 1, 1, 0, 2, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 1, 1]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "행렬 U :\n",
      "[[-0.24  0.75  0.   -0.62]\n",
      " [-0.51  0.44 -0.    0.74]\n",
      " [-0.83 -0.49 -0.   -0.27]\n",
      " [-0.   -0.    1.    0.  ]]\n",
      "행렬 U의 크기(shape) : (4, 4)\n"
     ]
    }
   ],
   "source": [
    "U,s,VT = np.linalg.svd(A, full_matrices= True)\n",
    "print('행렬 U :')\n",
    "print(U.round(2))\n",
    "print('행렬 U의 크기(shape) :',np.shape(U))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "특이값 벡터 :\n",
      "[2.69 2.05 1.73 0.77]\n",
      "특이값 벡터의 크기(shape) : (4,)\n"
     ]
    }
   ],
   "source": [
    "print('특이값 벡터 :')\n",
    "print(s.round(2))\n",
    "print('특이값 벡터의 크기(shape) :',np.shape(s))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "대각 행렬 S :\n",
      "[[2.69 0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   2.05 0.   0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   0.   1.73 0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   0.   0.   0.77 0.   0.   0.   0.   0.  ]]\n",
      "대각 행렬의 크기(shape) :\n",
      "(4, 9)\n"
     ]
    }
   ],
   "source": [
    "S = np.zeros((4,9))\n",
    "\n",
    "S[:4,:4] = np.diag(s)\n",
    "print('대각 행렬 S :')\n",
    "print(S.round(2))\n",
    "\n",
    "print('대각 행렬의 크기(shape) :')\n",
    "print(np.shape(S))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "직교행렬 VT :\n",
      "[[-0.   -0.31 -0.31 -0.28 -0.8  -0.09 -0.28 -0.   -0.  ]\n",
      " [ 0.   -0.24 -0.24  0.58 -0.26  0.37  0.58 -0.   -0.  ]\n",
      " [ 0.58 -0.    0.    0.   -0.    0.   -0.    0.58  0.58]\n",
      " [ 0.   -0.35 -0.35  0.16  0.25 -0.8   0.16 -0.   -0.  ]\n",
      " [-0.   -0.78 -0.01 -0.2   0.4   0.4  -0.2   0.    0.  ]\n",
      " [-0.29  0.31 -0.78 -0.24  0.23  0.23  0.01  0.14  0.14]\n",
      " [-0.29 -0.1   0.26 -0.59 -0.08 -0.08  0.66  0.14  0.14]\n",
      " [-0.5  -0.06  0.15  0.24 -0.05 -0.05 -0.19  0.75 -0.25]\n",
      " [-0.5  -0.06  0.15  0.24 -0.05 -0.05 -0.19 -0.25  0.75]]\n",
      "직교 행렬 VT의 크기(shape) :\n",
      "(9, 9)\n"
     ]
    }
   ],
   "source": [
    "print('직교행렬 VT :')\n",
    "print(VT.round(2))\n",
    "\n",
    "print('직교 행렬 VT의 크기(shape) :')\n",
    "print(np.shape(VT))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# allclose() : 2개의 행렬이 동일하면 True를 리턴.\n",
    "np.allclose(A, np.dot(np.dot(U,S),VT).round(2) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Truncated SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.68731789, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 2.04508425, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 1.73205081, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.77197992, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "대각 행렬 S :\n",
      "[[2.69 0.  ]\n",
      " [0.   2.05]]\n"
     ]
    }
   ],
   "source": [
    "# 특이값 상위 2개만 보존\n",
    "S = S[:2,:2]\n",
    "\n",
    "print('대각 행렬 S :')\n",
    "print(S.round(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "행렬 U :\n",
      "[[-0.24  0.75]\n",
      " [-0.51  0.44]\n",
      " [-0.83 -0.49]\n",
      " [-0.   -0.  ]]\n"
     ]
    }
   ],
   "source": [
    "U = U[:,:2]\n",
    "print('행렬 U :')\n",
    "print(U.round(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "직교행렬 VT :\n",
      "[[-0.   -0.31 -0.31 -0.28 -0.8  -0.09 -0.28 -0.   -0.  ]\n",
      " [ 0.   -0.24 -0.24  0.58 -0.26  0.37  0.58 -0.   -0.  ]]\n"
     ]
    }
   ],
   "source": [
    "VT = VT[:2,:]\n",
    "print('직교행렬 VT :')\n",
    "print(VT.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 1 0 1 1 0 0]\n",
      " [0 0 0 1 1 0 1 0 0]\n",
      " [0 1 1 0 2 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 1 1]]\n",
      "\n",
      "[[ 0.   -0.17 -0.17  1.08  0.12  0.62  1.08 -0.   -0.  ]\n",
      " [ 0.    0.2   0.2   0.91  0.86  0.45  0.91  0.    0.  ]\n",
      " [ 0.    0.93  0.93  0.03  2.05 -0.17  0.03  0.    0.  ]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.    0.    0.  ]]\n"
     ]
    }
   ],
   "source": [
    "A_prime = np.dot(np.dot(U,S),VT)\n",
    "print(A)\n",
    "print()\n",
    "print(A_prime.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 원래 문서 A의 크기 = (4,9). 즉, 4개의 문서와 각 문서는 9개의 단어로 구성되어 있다는 것.\n",
    "- 축소된 U 는 (4,2)의 크기. 이는 4개의 문서 * 토픽의 수(t)이다. 다시 말하면, 문서 각각을 2개의 값으로 표현하고 있는 것. U의 각 행은 잠재의미를 표현하기 위해 수치화된 각각의 `문서 벡터`.\n",
    "- 축소된 VT는 (2,9)의 크기. 이는 토픽의 수*단어의 개수의 크기다. VT의 각 열은 잠재 의미를 표현하기 위해 수치화된 각각의 `단어 벡터`라고 볼 수 있다.\n",
    "- 이 문서 벡터와 단어 벡터를 통해 다른 문서의 유사도, 다른 단어의 유사도, 단어로부터 문서의 유사도를 구하는 것이 가능하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) 뉴스그룹 데이터에 대한 이해"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "샘플의 수: 11314\n"
     ]
    }
   ],
   "source": [
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1,remove=('headers','footers','quotes'))\n",
    "documents = dataset.data\n",
    "print(f'샘플의 수: {len(documents)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DESCR', 'data', 'filenames', 'target', 'target_names']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([17,  0, 17, ...,  9,  4,  9])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 각 뉴스가 어느 그룹에 속하는지.\n",
    "dataset.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) 텍스트 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ① 문자 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df = pd.DataFrame({'documents':documents})\n",
    "\n",
    "# 특수문자 제거\n",
    "news_df['clean_doc'] = news_df['documents'].str.replace(\"[^a-zA-Z]\",\" \")\n",
    "\n",
    "# 길이가 3이하인 단어 제거(길이 짧은 단어 제거)\n",
    "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda row: ' '.join([w for w in row.split() if len(w)>3]))\n",
    "\n",
    "# 전체 단어에 대한 소문자 변환\n",
    "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda row: row.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"yeah, expect people read faq, etc. actually accept hard atheism? need little leap faith, jimmy. your logic runs steam! jim, sorry can't pity you, jim. sorry that have these feelings denial about faith need well, just pretend that will happily ever after anyway. maybe start newsgroup, alt.atheist.hard, won't bummin' much? bye-bye, jim. don't forget your flintstone's chewables! bake timmons,\""
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df['clean_doc'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ② 토큰화 & 불용어 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\TEMP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NLTK로부터 불용어 받아옴.\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [well, sure, about, story, seem, biased., what...\n",
       "1        [yeah,, expect, people, read, faq,, etc., actu...\n",
       "2        [although, realize, that, principle, your, str...\n",
       "3        [notwithstanding, legitimate, fuss, about, thi...\n",
       "4        [well,, will, have, change, scoring, playoff, ...\n",
       "                               ...                        \n",
       "11309    [danny, rubenstein,, israeli, journalist,, wil...\n",
       "11310                                                   []\n",
       "11311    [agree., home, runs, clemens, always, memorabl...\n",
       "11312    [used, deskjet, with, orange, micros, grappler...\n",
       "11313    [^^^^^^, argument, with, murphy., scared, hell...\n",
       "Name: clean_doc, Length: 11314, dtype: object"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 토큰화\n",
    "tokenized_doc = news_df['clean_doc'].apply(lambda x: x.split())\n",
    "tokenized_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [well, sure, story, seem, biased., disagree, s...\n",
       "1        [yeah,, expect, people, read, faq,, etc., actu...\n",
       "2        [although, realize, principle, strongest, poin...\n",
       "3        [notwithstanding, legitimate, fuss, proposal,,...\n",
       "4        [well,, change, scoring, playoff, pool., unfor...\n",
       "                               ...                        \n",
       "11309    [danny, rubenstein,, israeli, journalist,, spe...\n",
       "11310                                                   []\n",
       "11311    [agree., home, runs, clemens, always, memorabl...\n",
       "11312    [used, deskjet, orange, micros, grappler, syst...\n",
       "11313    [^^^^^^, argument, murphy., scared, hell, came...\n",
       "Name: clean_doc, Length: 11314, dtype: object"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words])\n",
    "tokenized_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3)TF-IDF 행렬 만들기\n",
    "- TfidfVectorizer는 기본적으로 토큰화가 되어있지 않은 텍스트 데이터를 입력으로 사용한다.\n",
    "- 따라서 역토큰화(Detokenization)을 진행해보겠음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ① 역토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>documents</th>\n",
       "      <th>clean_doc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Well i'm not sure about the story nad it did s...</td>\n",
       "      <td>well sure about story seem biased. what disagr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\nYeah, do you expect people to re...</td>\n",
       "      <td>yeah, expect people read faq, etc. actually ac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Although I realize that principle is not one o...</td>\n",
       "      <td>although realize that principle your strongest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Notwithstanding all the legitimate fuss about ...</td>\n",
       "      <td>notwithstanding legitimate fuss about this pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Well, I will have to change the scoring on my ...</td>\n",
       "      <td>well, will have change scoring playoff pool. u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11309</th>\n",
       "      <td>Danny Rubenstein, an Israeli journalist, will ...</td>\n",
       "      <td>danny rubenstein, israeli journalist, will spe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11310</th>\n",
       "      <td>\\n</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11311</th>\n",
       "      <td>\\nI agree.  Home runs off Clemens are always m...</td>\n",
       "      <td>agree. home runs clemens always memorable. kin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11312</th>\n",
       "      <td>I used HP DeskJet with Orange Micros Grappler ...</td>\n",
       "      <td>used deskjet with orange micros grappler syste...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11313</th>\n",
       "      <td>^^^^^^\\n...</td>\n",
       "      <td>^^^^^^ argument with murphy. scared hell when ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11314 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               documents  \\\n",
       "0      Well i'm not sure about the story nad it did s...   \n",
       "1      \\n\\n\\n\\n\\n\\n\\nYeah, do you expect people to re...   \n",
       "2      Although I realize that principle is not one o...   \n",
       "3      Notwithstanding all the legitimate fuss about ...   \n",
       "4      Well, I will have to change the scoring on my ...   \n",
       "...                                                  ...   \n",
       "11309  Danny Rubenstein, an Israeli journalist, will ...   \n",
       "11310                                                 \\n   \n",
       "11311  \\nI agree.  Home runs off Clemens are always m...   \n",
       "11312  I used HP DeskJet with Orange Micros Grappler ...   \n",
       "11313                                        ^^^^^^\\n...   \n",
       "\n",
       "                                               clean_doc  \n",
       "0      well sure about story seem biased. what disagr...  \n",
       "1      yeah, expect people read faq, etc. actually ac...  \n",
       "2      although realize that principle your strongest...  \n",
       "3      notwithstanding legitimate fuss about this pro...  \n",
       "4      well, will have change scoring playoff pool. u...  \n",
       "...                                                  ...  \n",
       "11309  danny rubenstein, israeli journalist, will spe...  \n",
       "11310                                                     \n",
       "11311  agree. home runs clemens always memorable. kin...  \n",
       "11312  used deskjet with orange micros grappler syste...  \n",
       "11313  ^^^^^^ argument with murphy. scared hell when ...  \n",
       "\n",
       "[11314 rows x 2 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 역토큰화\n",
    "detokenized_doc = []\n",
    "for i in range(len(news_df)):\n",
    "    t = ' '.join(tokenized_doc[i])\n",
    "    detokenized_doc.append(t)\n",
    "\n",
    "news_df['clean_doc'] = detokenized_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>documents</th>\n",
       "      <th>clean_doc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Well i'm not sure about the story nad it did s...</td>\n",
       "      <td>well sure story seem biased. disagree statemen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\nYeah, do you expect people to re...</td>\n",
       "      <td>yeah, expect people read faq, etc. actually ac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Although I realize that principle is not one o...</td>\n",
       "      <td>although realize principle strongest points, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Notwithstanding all the legitimate fuss about ...</td>\n",
       "      <td>notwithstanding legitimate fuss proposal, much...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Well, I will have to change the scoring on my ...</td>\n",
       "      <td>well, change scoring playoff pool. unfortunate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11309</th>\n",
       "      <td>Danny Rubenstein, an Israeli journalist, will ...</td>\n",
       "      <td>danny rubenstein, israeli journalist, speaking...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11310</th>\n",
       "      <td>\\n</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11311</th>\n",
       "      <td>\\nI agree.  Home runs off Clemens are always m...</td>\n",
       "      <td>agree. home runs clemens always memorable. kin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11312</th>\n",
       "      <td>I used HP DeskJet with Orange Micros Grappler ...</td>\n",
       "      <td>used deskjet orange micros grappler system6.0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11313</th>\n",
       "      <td>^^^^^^\\n...</td>\n",
       "      <td>^^^^^^ argument murphy. scared hell came last ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11314 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               documents  \\\n",
       "0      Well i'm not sure about the story nad it did s...   \n",
       "1      \\n\\n\\n\\n\\n\\n\\nYeah, do you expect people to re...   \n",
       "2      Although I realize that principle is not one o...   \n",
       "3      Notwithstanding all the legitimate fuss about ...   \n",
       "4      Well, I will have to change the scoring on my ...   \n",
       "...                                                  ...   \n",
       "11309  Danny Rubenstein, an Israeli journalist, will ...   \n",
       "11310                                                 \\n   \n",
       "11311  \\nI agree.  Home runs off Clemens are always m...   \n",
       "11312  I used HP DeskJet with Orange Micros Grappler ...   \n",
       "11313                                        ^^^^^^\\n...   \n",
       "\n",
       "                                               clean_doc  \n",
       "0      well sure story seem biased. disagree statemen...  \n",
       "1      yeah, expect people read faq, etc. actually ac...  \n",
       "2      although realize principle strongest points, w...  \n",
       "3      notwithstanding legitimate fuss proposal, much...  \n",
       "4      well, change scoring playoff pool. unfortunate...  \n",
       "...                                                  ...  \n",
       "11309  danny rubenstein, israeli journalist, speaking...  \n",
       "11310                                                     \n",
       "11311  agree. home runs clemens always memorable. kin...  \n",
       "11312  used deskjet orange micros grappler system6.0....  \n",
       "11313  ^^^^^^ argument murphy. scared hell came last ...  \n",
       "\n",
       "[11314 rows x 2 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ② TF-IDF 행렬 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF의 행렬의 크기: (11314, 1000)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=1000, # 상위 1000개 단어 보존 \n",
    "                             max_df = 0.5, smooth_idf = True) # max_df = 단어가 너무 자주 나타나는 경우(50%이상의 문서에 나타나는 경우) 해당 단어 무시.\n",
    "\n",
    "X = vectorizer.fit_transform(news_df['clean_doc']) # X = TF-IDF 가중치가 부여된 문서-단어 행렬\n",
    "\n",
    "# TF-IDF의 행렬 크기 확인\n",
    "print(f'TF-IDF의 행렬의 크기: {X.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) 토픽 모델링(Topic Modeling)\n",
    "- 이제 TF-IDF행렬을 다수의 행렬로 분해.\n",
    "- Truncated SVD 사용."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd_model = TruncatedSVD(n_components=20, algorithm='randomized',\n",
    "                         n_iter=100, random_state=122)\n",
    "\n",
    "svd_model.fit(X)\n",
    "len(svd_model.components_) # svd_model.components_ = VT에 해당됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 1000)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(svd_model.components_) # 토픽의 수(t) * 단어의 수의 크기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([166,  48, 397,  45,  33,  32, 385,  57, 143,  23,  44,  47, 131,\n",
       "       203,  37,   6,  24, 948,  38, 717,  56, 167,  49, 522, 664,   5,\n",
       "       266, 398, 265, 975, 142, 130, 596, 819,  13, 594,  74, 851, 149,\n",
       "        43, 558,  36, 508, 477,   4,  53,  31, 855,   2,  40,  46, 879,\n",
       "         3,  75, 330, 226,  27,  35, 123,  28, 551, 756,  60,  54, 776,\n",
       "       579, 391,  20,  52,  26,  18,  17, 109, 921, 757, 947, 835, 712,\n",
       "       916, 348, 104, 580, 824, 933, 872,  86, 994,  29, 284, 628, 843,\n",
       "       355, 555, 973,  11, 810, 865, 301,  85, 237, 816, 383, 242, 635,\n",
       "        16, 624, 804,  39, 323, 868, 638, 278, 907, 151,  19, 122,  51,\n",
       "       436, 914, 711, 964, 840, 966, 146,  67, 705, 633, 112, 595,  42,\n",
       "       749, 359, 119, 120,  79, 124, 772, 715, 834, 369, 320, 475, 615,\n",
       "       465,  84, 976, 632, 230, 903,  55,  68, 541, 409, 556, 327, 787,\n",
       "       228, 663, 754, 926, 170, 100, 229, 642,  81,  70, 963, 862, 331,\n",
       "        50, 428, 967, 125, 445, 238, 252,  92, 353, 103, 767, 899, 920,\n",
       "       692, 299, 478, 713, 113, 660, 317, 708, 790, 148, 280, 741, 334,\n",
       "       289, 452, 821, 108,  64,  97, 106, 251, 915, 847, 953, 261, 696,\n",
       "       739, 496, 244, 589, 940, 676, 584, 381, 160, 172, 537, 768, 651,\n",
       "       115, 648, 554, 619, 101, 740, 565, 923, 724, 192, 782, 471, 225,\n",
       "       591, 206, 597,  88, 287, 784, 904, 970, 968, 729, 337, 703, 366,\n",
       "       499, 313, 343, 954, 322, 805, 791,  78, 506, 304, 750, 250, 993,\n",
       "       693, 886, 734, 918, 856, 137, 190, 998, 336, 587, 662,  80, 709,\n",
       "       990, 243, 121,  93, 571, 490, 168, 204, 399, 306, 743, 257, 520,\n",
       "        82, 344, 174, 792, 631, 439, 214, 460, 379, 231, 958, 293, 135,\n",
       "       396, 463, 532, 536, 259, 305, 358, 329, 492, 295,  58, 303, 882,\n",
       "       667, 262, 647, 222, 686, 751, 722, 752, 362, 302, 748, 453, 925,\n",
       "        21, 405, 766, 382, 643,  12, 285, 850, 509, 223, 138, 578, 561,\n",
       "       514, 669, 544, 332, 769, 678, 494, 636, 566,   9, 625, 312, 193,\n",
       "       564,  30, 425, 935, 268,  76, 480, 208, 158, 688,  41, 390, 152,\n",
       "       600, 677, 707, 956, 753, 297, 652, 128, 159, 179, 576, 735, 491,\n",
       "       572, 661, 282,  96, 841, 838, 911, 742, 983, 539, 905, 294, 414,\n",
       "       276, 773, 815, 809, 370, 395, 525, 403, 592,  14, 456, 136, 608,\n",
       "       163, 863,  65, 955, 292, 932, 357,  34, 682, 169,  91, 211, 132,\n",
       "       909, 762, 644, 946, 620, 837,  10, 111, 189, 568, 928, 269, 283,\n",
       "       987, 878, 777, 233, 974, 656, 869,  15, 449, 808, 825, 286, 209,\n",
       "        63, 326, 654, 457, 716, 110, 341, 885, 602, 999, 420, 354, 446,\n",
       "       833, 421, 590, 874, 733, 941, 799,  25, 485, 697, 978, 574, 623,\n",
       "       224, 563, 891, 690, 549, 521, 760, 806, 200, 412, 410, 542, 388,\n",
       "       666, 870, 906, 858, 737, 826, 640, 732, 464, 450, 258, 432, 738,\n",
       "       360, 372, 702, 607, 288, 501, 377, 102, 679, 934, 197, 240, 349,\n",
       "       701, 852, 930, 182, 548, 606, 866, 861, 924, 234, 347, 187, 367,\n",
       "       695, 127, 881, 575, 621, 435, 959, 281, 761, 316, 593, 550, 473,\n",
       "        89, 952, 467, 650, 487, 793, 604, 689, 745, 196, 277, 800, 162,\n",
       "       502, 458, 384, 260, 504, 969, 376,   8, 949, 977, 256, 781, 630,\n",
       "       534, 184, 161, 469, 440, 533, 333, 497, 725, 529, 164, 803, 264,\n",
       "       553, 639, 133, 610, 645,  61, 363, 416, 598, 246, 527, 857, 573,\n",
       "        77, 601, 888, 498, 854, 961, 375, 482, 646, 822, 221, 718, 614,\n",
       "       552, 945, 723, 321, 205, 674, 483, 658, 801, 215, 117, 443, 875,\n",
       "       655, 345, 817, 883, 156, 296, 374, 670, 965, 848, 629, 583, 939,\n",
       "       342, 758, 165, 991, 212, 500, 765, 307, 241, 273, 479,  98, 466,\n",
       "       328, 314, 599, 311, 315, 371, 795, 684, 979, 641, 887, 818, 680,\n",
       "       612, 118, 889, 236,  22, 424, 516, 481, 177, 710, 419, 569, 672,\n",
       "       350, 605, 150, 675, 275, 210, 936, 873, 451, 771, 981, 807, 116,\n",
       "       543, 759, 649, 489, 706, 618, 434, 448, 175, 415, 849, 157, 418,\n",
       "       802, 433, 154, 814,   7, 227, 736, 198, 582, 444, 352, 511, 107,\n",
       "       798, 755, 588, 232, 474, 279, 392, 783,  95, 581, 912, 853, 325,\n",
       "       318, 876, 147, 213, 902, 247, 895,   0, 627, 253, 180, 988, 960,\n",
       "       423, 507, 335,  72, 626, 764, 744, 775, 747, 877, 989, 270, 185,\n",
       "       429, 300,   1, 380, 917, 462, 310, 505,  94, 239, 845, 786, 938,\n",
       "       951, 401, 671, 488, 271,  83, 727, 788, 812, 447, 248, 871, 339,\n",
       "       476, 570, 785, 531, 356, 201,  62, 468, 346, 796, 153, 274, 831,\n",
       "       813, 470, 778, 426, 207, 513, 622, 811, 178, 495, 685, 191, 194,\n",
       "       441, 202, 547, 217, 830, 962, 927, 459, 984, 860, 929, 249, 827,\n",
       "       290, 309, 183, 842, 105, 389, 922, 900, 519, 538, 186, 557, 779,\n",
       "       368, 254, 919, 493, 535, 616,  99, 829, 832, 577, 980, 173,  69,\n",
       "       908, 828, 586, 220, 720, 361, 901, 216, 417, 272, 437, 298, 263,\n",
       "       407, 609, 199, 378, 144,  73, 338,  87, 659, 188, 394, 611, 524,\n",
       "       373, 637, 668, 944, 562, 114, 820, 245, 413, 957, 546, 567, 387,\n",
       "       613, 438, 985, 839, 145, 503, 714, 340, 134,  71, 898, 585, 694,\n",
       "       770, 155, 971, 129, 943,  90, 665, 454, 859, 844, 864, 634, 780,\n",
       "       365, 195, 731, 657, 484, 797, 746, 461, 997, 235, 517, 691, 324,\n",
       "       846, 472, 913, 442, 515, 721, 411, 789, 393, 171, 431, 267, 880,\n",
       "       700, 560,  59, 510, 836, 823, 896, 681, 992, 126, 427, 400, 794,\n",
       "       728, 422, 351, 559, 291, 364, 617, 910, 986, 408, 386, 683, 176,\n",
       "       687, 530, 704,  66, 455, 140, 255, 219, 218, 884, 181, 406, 528,\n",
       "       518, 726, 526, 698, 719, 308, 892, 540, 523, 673, 430, 141, 996,\n",
       "       893, 937, 867, 402, 139, 774, 972, 995, 982, 931, 699, 730, 603,\n",
       "       319, 950, 763, 545, 942, 890, 897, 404, 894, 653, 486, 512],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd_model.components_[0].argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([512, 486, 653, 894, 404], dtype=int64)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd_model.components_[0].argsort()[:-5-1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: [('like', 0.2085), ('know', 0.19656), ('people', 0.1912), ('think', 0.17523), ('good', 0.14902)]\n",
      "Topic 2: [('thanks', 0.31338), ('windows', 0.27934), ('card', 0.17289), ('drive', 0.16141), ('mail', 0.14507)]\n",
      "Topic 3: [('game', 0.36553), ('team', 0.3133), ('year', 0.28465), ('games', 0.23048), ('season', 0.17026)]\n",
      "Topic 4: [('edu', 0.50341), ('thanks', 0.25409), ('mail', 0.1758), ('com', 0.11498), ('email', 0.11166)]\n",
      "Topic 5: [('edu', 0.49934), ('drive', 0.24972), ('com', 0.10645), ('sale', 0.10616), ('soon', 0.09199)]\n",
      "Topic 6: [('drive', 0.40102), ('thanks', 0.34667), ('know', 0.27592), ('scsi', 0.13765), ('mail', 0.11332)]\n",
      "Topic 7: [('chip', 0.21565), ('government', 0.20249), ('like', 0.17148), ('encryption', 0.14654), ('clipper', 0.14478)]\n",
      "Topic 8: [('like', 0.64668), ('edu', 0.31439), ('bike', 0.12683), ('know', 0.12403), ('think', 0.11547)]\n",
      "Topic 9: [('card', 0.3572), ('sale', 0.17543), ('00', 0.17496), ('video', 0.16994), ('good', 0.15574)]\n",
      "Topic 10: [('card', 0.45093), ('people', 0.35248), ('know', 0.26213), ('video', 0.20438), ('edu', 0.18018)]\n",
      "Topic 11: [('like', 0.53784), ('people', 0.29344), ('windows', 0.14585), ('drive', 0.14199), ('game', 0.138)]\n",
      "Topic 12: [('think', 0.73098), ('thanks', 0.19253), ('people', 0.12027), ('mail', 0.11436), ('good', 0.11292)]\n",
      "Topic 13: [('know', 0.21625), ('chip', 0.21482), ('jesus', 0.19227), ('game', 0.1597), ('think', 0.15475)]\n",
      "Topic 14: [('know', 0.33277), ('good', 0.31777), ('people', 0.22055), ('windows', 0.21856), ('file', 0.20588)]\n",
      "Topic 15: [('good', 0.38909), ('chip', 0.22814), ('people', 0.15098), ('clipper', 0.14368), ('encryption', 0.14025)]\n",
      "Topic 16: [('com', 0.6569), ('ve', 0.34058), ('people', 0.20544), ('seen', 0.0979), ('list', 0.09405)]\n",
      "Topic 17: [('ve', 0.38594), ('space', 0.30952), ('people', 0.28754), ('seen', 0.13734), ('heard', 0.13676)]\n",
      "Topic 18: [('good', 0.32101), ('israel', 0.25517), ('card', 0.24702), ('com', 0.1984), ('israeli', 0.13667)]\n",
      "Topic 19: [('ve', 0.54441), ('israel', 0.20771), ('seen', 0.17641), ('00', 0.14313), ('heard', 0.13857)]\n",
      "Topic 20: [('time', 0.34123), ('bike', 0.28176), ('right', 0.25465), ('file', 0.20011), ('need', 0.19803)]\n"
     ]
    }
   ],
   "source": [
    "terms = vectorizer.get_feature_names_out() \n",
    "# 이전의 TF-IDF 변환에 사용한 vectorizer에서 단어집합(1000개단어)을 얻어옴.\n",
    "# TF-IDF 변환에 사용된 단어들을 포함하는 리스트 = terms\n",
    "\n",
    "def get_topics(components, feature_names, n=5):\n",
    "    for idx, topic in enumerate(components):\n",
    "        print(\"Topic %d:\" % (idx+1), [(feature_names[i], topic[i].round(5)) for i in topic.argsort()[:-n - 1:-1]])\n",
    "get_topics(svd_model.components_,terms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
