{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f41433c",
   "metadata": {},
   "source": [
    "# 사이킷런 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "481868fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /home/ubuntu/miniconda3/envs/spark_env/lib/python3.8/site-packages (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /home/ubuntu/.local/lib/python3.8/site-packages (from scikit-learn) (1.24.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ubuntu/miniconda3/envs/spark_env/lib/python3.8/site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/ubuntu/miniconda3/envs/spark_env/lib/python3.8/site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /home/ubuntu/miniconda3/envs/spark_env/lib/python3.8/site-packages (from scikit-learn) (1.10.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ce0aedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "602cc944",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['setosa', 'versicolor', 'virginica'], dtype='<U10')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87211330",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal_length  sepal_width  petal_length  petal_width  target\n",
       "0             5.1          3.5           1.4          0.2       0\n",
       "1             4.9          3.0           1.4          0.2       0\n",
       "2             4.7          3.2           1.3          0.2       0\n",
       "3             4.6          3.1           1.5          0.2       0\n",
       "4             5.0          3.6           1.4          0.2       0\n",
       "..            ...          ...           ...          ...     ...\n",
       "145           6.7          3.0           5.2          2.3       2\n",
       "146           6.3          2.5           5.0          1.9       2\n",
       "147           6.5          3.0           5.2          2.0       2\n",
       "148           6.2          3.4           5.4          2.3       2\n",
       "149           5.9          3.0           5.1          1.8       2\n",
       "\n",
       "[150 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# iris datasets 로딩\n",
    "iris = load_iris()\n",
    "\n",
    "iris_data  = iris.data # feature\n",
    "iris_label = iris.target # label\n",
    "\n",
    "iris_columns = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"]\n",
    "iris_pdf = pd.DataFrame(iris_data, columns=iris_columns)\n",
    "iris_pdf['target'] = iris_label\n",
    "iris_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1d6a55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_pdf.to_csv('./data/iris.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2847562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0]\n"
     ]
    }
   ],
   "source": [
    "# 데이터 분할 및 모델 생성\n",
    "# from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.tree import DecisionTreeClassifier # Estimator\n",
    "from sklearn.model_selection import train_test_split # RandomSpliter\n",
    "\n",
    "X_train, X_test, t_train, t_test = train_test_split(\n",
    "    iris_data,\n",
    "    iris_label,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "tree_clf = DecisionTreeClassifier()\n",
    "tree_clf.fit(X_train, t_train) # 훈련! tree_clf 모델 자체에서 훈련이 일어나게 된다.\n",
    "\n",
    "pred = tree_clf.predict(X_test)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a2e406",
   "metadata": {},
   "source": [
    "# Spark ML 사용하기\n",
    "- 100% dataframe 기반\n",
    "- scikit learn은 numpy, pandas 기반"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9b0323f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/ubuntu/spark-3.2.4/jars/spark-unsafe_2.12-3.2.4.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/06/13 04:30:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/06/13 04:30:25 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/06/13 04:30:25 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "23/06/13 04:30:25 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "23/06/13 04:30:25 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master('local').appName('tree-clf').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f680d721",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|target|\n",
      "+------------+-----------+------------+-----------+------+\n",
      "|         5.1|        3.5|         1.4|        0.2|     0|\n",
      "|         4.9|        3.0|         1.4|        0.2|     0|\n",
      "|         4.7|        3.2|         1.3|        0.2|     0|\n",
      "|         4.6|        3.1|         1.5|        0.2|     0|\n",
      "|         5.0|        3.6|         1.4|        0.2|     0|\n",
      "+------------+-----------+------------+-----------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iris_filepath = \"/home/ubuntu/working/spark-examples/data/iris.csv\"\n",
    "iris_sdf = spark.read.csv(f'file://{iris_filepath}',\n",
    "                         inferSchema = True,\n",
    "                         header = True)\n",
    "iris_sdf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67f2f516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomSplit 메소드를 활용해 훈련/ 테스트 데이터 세트 분할\n",
    "\n",
    "train_sdf, test_sdf = iris_sdf.randomSplit([0.8,0.2], seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba9581d",
   "metadata": {},
   "source": [
    "## cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b9565fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|target|\n",
      "+------------+-----------+------------+-----------+------+\n",
      "|         4.3|        3.0|         1.1|        0.1|     0|\n",
      "|         4.4|        2.9|         1.4|        0.2|     0|\n",
      "|         4.4|        3.2|         1.3|        0.2|     0|\n",
      "|         4.5|        2.3|         1.3|        0.3|     0|\n",
      "|         4.6|        3.1|         1.5|        0.2|     0|\n",
      "+------------+-----------+------------+-----------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 훈련 데이터 세트는 어떻게 변환이 되어도 하나만 존재하는게 좋다.\n",
    "# -> 모델을 여러개 사용해서 변환이 되는 상황\n",
    "# 여러개의 훈련 데이터 프레임이 생성되는 것을 방지해야 한다\n",
    "\n",
    "# 훈련 데이터가 모델에 들어가게 되면 transform이 일어나게 된다.\n",
    "# 여러번의 훈련을 거치게 되면, transform이 여러 번 일어나게 된다.\n",
    "# ---> train_sdf가 메모리 내에 여러개 똑같은 것이 생길 수 있다.\n",
    "\n",
    "# 따.라.서!!! 훈련 직전에 사용할 데이터는 캐싱을 하는게 좋다.\n",
    "train_sdf.cache()\n",
    "train_sdf.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e90ce61",
   "metadata": {},
   "source": [
    "## VectorAssembler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29edd5eb",
   "metadata": {},
   "source": [
    "- 이 때 `벡터 어셈블러` 를 이용해서 한 row에 있는 데이터를 묶어줄 것임.\n",
    "- 현재에는 sepal_length, sepal_width..등 데이터가 모두 떨어져 있는 상태"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b0e647",
   "metadata": {},
   "source": [
    "`VectorAssembler` 를 이용해 모든 feature 컬럼을 하나의 feature vector로 만든다.(행벡터)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbd73ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+------+-----------------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|target|         features|\n",
      "+------------+-----------+------------+-----------+------+-----------------+\n",
      "|         4.3|        3.0|         1.1|        0.1|     0|[4.3,3.0,1.1,0.1]|\n",
      "|         4.4|        2.9|         1.4|        0.2|     0|[4.4,2.9,1.4,0.2]|\n",
      "|         4.4|        3.2|         1.3|        0.2|     0|[4.4,3.2,1.3,0.2]|\n",
      "|         4.5|        2.3|         1.3|        0.3|     0|[4.5,2.3,1.3,0.3]|\n",
      "|         4.6|        3.1|         1.5|        0.2|     0|[4.6,3.1,1.5,0.2]|\n",
      "+------------+-----------+------------+-----------+------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# VectorAssembler 로 데이터프레임에 있는 데이터를 하나의 행벡터로 합쳐준다.\n",
    "\n",
    "\n",
    "# inputCols = 합칠 컬럼의 목록들\n",
    "# outputCol = 데이터가 합쳐진 컬럼의 이름. assemble이 완료된 컬럼\n",
    "\n",
    "iris_columns = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"]\n",
    "\n",
    "# inputCols에 집어넣은 컬럼들이 outputCol인 features라는 이름으로 벡터로 묶일 것\n",
    "vec_assembler = VectorAssembler(inputCols=iris_columns, outputCol = 'features')\n",
    "\n",
    "# VectorAssembler Transform\n",
    "train_feature_vector_sdf = vec_assembler.transform(train_sdf)\n",
    "train_feature_vector_sdf.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49198847",
   "metadata": {},
   "source": [
    "# Estimator\n",
    "\n",
    "spark ML의 모델은 추정기(Estimator)지만, 데이터를 변환시키는 Transformer에 해당한다.\n",
    "- train데이터를 받아서 예측값(prediction)으로 변환을 시키는 transform 과정이 일어나기 때문"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245884ca",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5f266841",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.ml.classification.DecisionTreeClassifier"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "# 모델 생성: \"데이터 프레임의 어떤 컬럼\"의 데이터를 이용해서 학습을 할지 결정 지어줘야 한다.\n",
    "dt = DecisionTreeClassifier(\n",
    "    featuresCol = 'features',\n",
    "    labelCol = 'target',\n",
    "    maxDepth = 2,\n",
    "    \n",
    ")\n",
    "type(dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "505774ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.ml.classification.DecisionTreeClassificationModel"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 학슴. fit() 메소드를 이용하여 학습을 수행하고, 그 결과를 ML 모델로 변환한다.\n",
    "dt_model = dt.fit(train_feature_vector_sdf)\n",
    "type(dt_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3522ddb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|target|\n",
      "+------------+-----------+------------+-----------+------+\n",
      "|         4.4|        3.0|         1.3|        0.2|     0|\n",
      "|         4.6|        3.2|         1.4|        0.2|     0|\n",
      "|         4.6|        3.6|         1.0|        0.2|     0|\n",
      "|         4.8|        3.1|         1.6|        0.2|     0|\n",
      "|         4.9|        3.1|         1.5|        0.1|     0|\n",
      "+------------+-----------+------------+-----------+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+------------+-----------+------------+-----------+------+-----------------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|target|         features|\n",
      "+------------+-----------+------------+-----------+------+-----------------+\n",
      "|         4.4|        3.0|         1.3|        0.2|     0|[4.4,3.0,1.3,0.2]|\n",
      "|         4.6|        3.2|         1.4|        0.2|     0|[4.6,3.2,1.4,0.2]|\n",
      "|         4.6|        3.6|         1.0|        0.2|     0|[4.6,3.6,1.0,0.2]|\n",
      "|         4.8|        3.1|         1.6|        0.2|     0|[4.8,3.1,1.6,0.2]|\n",
      "|         4.9|        3.1|         1.5|        0.1|     0|[4.9,3.1,1.5,0.1]|\n",
      "+------------+-----------+------------+-----------+------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 테스트 데이터 예측\n",
    "test_sdf.show(5)\n",
    "\n",
    "# 훈련 데이터에서 적용시켰던 Transformer를 테스트 세트에다가도 그대로 적용시킨다.\n",
    "test_feature_vector_sdf = vec_assembler.transform(test_sdf)\n",
    "test_feature_vector_sdf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9a2f203c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+------+-----------------+--------------+--------------------+----------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|target|         features| rawPrediction|         probability|prediction|\n",
      "+------------+-----------+------------+-----------+------+-----------------+--------------+--------------------+----------+\n",
      "|         4.4|        3.0|         1.3|        0.2|     0|[4.4,3.0,1.3,0.2]|[39.0,0.0,0.0]|       [1.0,0.0,0.0]|       0.0|\n",
      "|         4.6|        3.2|         1.4|        0.2|     0|[4.6,3.2,1.4,0.2]|[39.0,0.0,0.0]|       [1.0,0.0,0.0]|       0.0|\n",
      "|         4.6|        3.6|         1.0|        0.2|     0|[4.6,3.6,1.0,0.2]|[39.0,0.0,0.0]|       [1.0,0.0,0.0]|       0.0|\n",
      "|         4.8|        3.1|         1.6|        0.2|     0|[4.8,3.1,1.6,0.2]|[39.0,0.0,0.0]|       [1.0,0.0,0.0]|       0.0|\n",
      "|         4.9|        3.1|         1.5|        0.1|     0|[4.9,3.1,1.5,0.1]|[39.0,0.0,0.0]|       [1.0,0.0,0.0]|       0.0|\n",
      "|         5.0|        2.3|         3.3|        1.0|     1|[5.0,2.3,3.3,1.0]|[0.0,39.0,1.0]|   [0.0,0.975,0.025]|       1.0|\n",
      "|         5.0|        3.5|         1.3|        0.3|     0|[5.0,3.5,1.3,0.3]|[39.0,0.0,0.0]|       [1.0,0.0,0.0]|       0.0|\n",
      "|         5.1|        3.5|         1.4|        0.2|     0|[5.1,3.5,1.4,0.2]|[39.0,0.0,0.0]|       [1.0,0.0,0.0]|       0.0|\n",
      "|         5.3|        3.7|         1.5|        0.2|     0|[5.3,3.7,1.5,0.2]|[39.0,0.0,0.0]|       [1.0,0.0,0.0]|       0.0|\n",
      "|         5.4|        3.0|         4.5|        1.5|     1|[5.4,3.0,4.5,1.5]|[0.0,39.0,1.0]|   [0.0,0.975,0.025]|       1.0|\n",
      "|         5.4|        3.4|         1.5|        0.4|     0|[5.4,3.4,1.5,0.4]|[39.0,0.0,0.0]|       [1.0,0.0,0.0]|       0.0|\n",
      "|         5.4|        3.7|         1.5|        0.2|     0|[5.4,3.7,1.5,0.2]|[39.0,0.0,0.0]|       [1.0,0.0,0.0]|       0.0|\n",
      "|         5.4|        3.9|         1.7|        0.4|     0|[5.4,3.9,1.7,0.4]|[39.0,0.0,0.0]|       [1.0,0.0,0.0]|       0.0|\n",
      "|         5.5|        2.5|         4.0|        1.3|     1|[5.5,2.5,4.0,1.3]|[0.0,39.0,1.0]|   [0.0,0.975,0.025]|       1.0|\n",
      "|         5.6|        2.9|         3.6|        1.3|     1|[5.6,2.9,3.6,1.3]|[0.0,39.0,1.0]|   [0.0,0.975,0.025]|       1.0|\n",
      "|         5.7|        2.9|         4.2|        1.3|     1|[5.7,2.9,4.2,1.3]|[0.0,39.0,1.0]|   [0.0,0.975,0.025]|       1.0|\n",
      "|         5.8|        2.7|         5.1|        1.9|     2|[5.8,2.7,5.1,1.9]|[0.0,5.0,42.0]|[0.0,0.1063829787...|       2.0|\n",
      "|         6.3|        2.5|         4.9|        1.5|     1|[6.3,2.5,4.9,1.5]|[0.0,5.0,42.0]|[0.0,0.1063829787...|       2.0|\n",
      "|         6.4|        3.1|         5.5|        1.8|     2|[6.4,3.1,5.5,1.8]|[0.0,5.0,42.0]|[0.0,0.1063829787...|       2.0|\n",
      "|         6.5|        3.0|         5.2|        2.0|     2|[6.5,3.0,5.2,2.0]|[0.0,5.0,42.0]|[0.0,0.1063829787...|       2.0|\n",
      "+------------+-----------+------------+-----------+------+-----------------+--------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 예측\n",
    "predictions = dt_model.transform(test_feature_vector_sdf)\n",
    "predictions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da6b9a3",
   "metadata": {},
   "source": [
    "- rawPrediction: 각 붓꽃 종류에 대한 예측 점수\n",
    "- probability: 각 붓꽃 종류에 대한 예측 확률\n",
    "- prediction: 예측된 붓꽃 종류의 레이블\n",
    "    - 즉, iris.target_names를 보면 (['setosa', 'versicolor', 'virginica'],  꽃 종류는 이렇게 세가지\n",
    "    - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0175d1be",
   "metadata": {},
   "source": [
    "- `rawPrediction`: 머신러닝 모델 알고리즘 별로 다를 수 있다.\n",
    "   - 머시널닝 알고리즘에 의해 계산된 값\n",
    "   - 값에 대한 정확한 의미는 없음.\n",
    "   - `LogisticRegression` 의 경우 예측  label 별로, 예측 수행 전 `sigmoid` 함수 적용 전 값\n",
    "       - y hat = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ab05b4",
   "metadata": {},
   "source": [
    "# 모델 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "707388f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9583333333333334"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator_accuracy = MulticlassClassificationEvaluator(\n",
    "    labelCol='target',\n",
    "    predictionCol='prediction',\n",
    "    metricName = 'accuracy'\n",
    ")\n",
    "\n",
    "accuracy = evaluator_accuracy.evaluate(predictions)\n",
    "accuracy # maxDepth 수 많아지면 정확도 올라가는 경향?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265e3e36",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0181b071",
   "metadata": {},
   "source": [
    "- 내 답"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6ec8448a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# 벡터 어셈블\n",
    "# 모델 생성\n",
    "lr = LogisticRegression(\n",
    "        featuresCol='features',\n",
    "        labelCol='target', maxIter=10)\n",
    "\n",
    "# 모델 학습\n",
    "lr_model = lr.fit(train_feature_vector_sdf)\n",
    "\n",
    "# 테스트 데이터 변환\n",
    "test_feature_vector_lr = vec_assembler.transform(test_sdf)\n",
    "\n",
    "# 예측\n",
    "lr_pred = lr_model.transform(test_feature_vector_lr)\n",
    "\n",
    "# 모델 평가\n",
    "lr_eval = MulticlassClassificationEvaluator(\n",
    "        labelCol='target',\n",
    "        predictionCol='prediction',\n",
    "        metricName = 'accuracy'\n",
    ")\n",
    "\n",
    "accuracy_lr = lr_eval.evaluate(lr_pred)\n",
    "accuracy_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c373f5",
   "metadata": {},
   "source": [
    "- 강사님 답"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0f5f1d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+------+-----------------+--------------------+--------------------+----------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|target|         features|       rawPrediction|         probability|prediction|\n",
      "+------------+-----------+------------+-----------+------+-----------------+--------------------+--------------------+----------+\n",
      "|         4.4|        3.0|         1.3|        0.2|     0|[4.4,3.0,1.3,0.2]|[18.6086266693526...|[0.99997762791224...|       0.0|\n",
      "|         4.6|        3.2|         1.4|        0.2|     0|[4.6,3.2,1.4,0.2]|[18.8180066107263...|[0.99997581287298...|       0.0|\n",
      "|         4.6|        3.6|         1.0|        0.2|     0|[4.6,3.6,1.0,0.2]|[22.6963845270307...|[0.99999942608846...|       0.0|\n",
      "|         4.8|        3.1|         1.6|        0.2|     0|[4.8,3.1,1.6,0.2]|[16.7506644665745...|[0.99971232954776...|       0.0|\n",
      "|         4.9|        3.1|         1.5|        0.1|     0|[4.9,3.1,1.5,0.1]|[17.3393987944099...|[0.99969323067671...|       0.0|\n",
      "+------------+-----------+------------+-----------+------+-----------------+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "정확도 1.0\n"
     ]
    }
   ],
   "source": [
    "# LogisticRegression 사용해 보기[실습]\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# ML 알고리즘 객체 생성\n",
    "lr = LogisticRegression(featuresCol='features', labelCol='target', maxIter=10)\n",
    "\n",
    "lr_model = lr.fit(train_feature_vector_sdf)\n",
    "\n",
    "predictions = lr_model.transform(test_feature_vector_sdf)\n",
    "predictions.show(5)\n",
    "\n",
    "accuracy = evaluator_accuracy.evaluate(predictions)\n",
    "print(\"정확도\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b050923",
   "metadata": {},
   "source": [
    "# 파이프라인 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "af08b25b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.ml.pipeline.Pipeline"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "iris_columns = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
    "\n",
    "# Pipeline은 개별 변환 및 모델 학습 작업을 각각의 stage로 정의해서 파이프라인에 순서대로 등록\n",
    "# Pipeline.fit() 메소드를 활용해서 순서대로 연결된 스테이지 작업을 일괄적으로 수행\n",
    "# Pipeline.fit()의 결과물은 PipelineModel로 반환이 된다.\n",
    "# PipelineModel에서 예측 작업을 transform()으로 수행\n",
    "\n",
    "# 첫 번째 stage는 Feature Vectorization을 위한 VectorAssembler\n",
    "stage_1 = VectorAssembler(inputCols=iris_columns, outputCol = 'features')\n",
    "\n",
    "# 두번째 stage는 학습을 위한 모델 생성\n",
    "stage_2 = DecisionTreeClassifier(featuresCol='features', labelCol='target', maxDepth = 3)\n",
    "\n",
    "# 리스트를 활용해 stage를 순서대로 배치\n",
    "stages = [ stage_1, stage_2 ]\n",
    "\n",
    "# 파이프라인에 등록\n",
    "pipeline = Pipeline(stages = stages)\n",
    "type(pipeline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e1e9f9ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.ml.pipeline.PipelineModel"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_model = pipeline.fit(train_sdf)\n",
    "type(pipeline_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d70c510a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+------+-----------------+--------------+-------------+----------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|target|         features| rawPrediction|  probability|prediction|\n",
      "+------------+-----------+------------+-----------+------+-----------------+--------------+-------------+----------+\n",
      "|         4.4|        3.0|         1.3|        0.2|     0|[4.4,3.0,1.3,0.2]|[39.0,0.0,0.0]|[1.0,0.0,0.0]|       0.0|\n",
      "|         4.6|        3.2|         1.4|        0.2|     0|[4.6,3.2,1.4,0.2]|[39.0,0.0,0.0]|[1.0,0.0,0.0]|       0.0|\n",
      "|         4.6|        3.6|         1.0|        0.2|     0|[4.6,3.6,1.0,0.2]|[39.0,0.0,0.0]|[1.0,0.0,0.0]|       0.0|\n",
      "|         4.8|        3.1|         1.6|        0.2|     0|[4.8,3.1,1.6,0.2]|[39.0,0.0,0.0]|[1.0,0.0,0.0]|       0.0|\n",
      "|         4.9|        3.1|         1.5|        0.1|     0|[4.9,3.1,1.5,0.1]|[39.0,0.0,0.0]|[1.0,0.0,0.0]|       0.0|\n",
      "+------------+-----------+------------+-----------+------+-----------------+--------------+-------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 파이프라인을 통해서 테스트 세트 예측 \n",
    "predictions = pipeline_model.transform(test_sdf)\n",
    "predictions.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0805d2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36fb7fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
