{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNQ32QRQUes8D5UxY12JRFQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JeeyeonKim00/TIL/blob/master/02_1_%ED%86%A0%ED%81%B0%ED%99%94.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. 단어 토큰화(word tokenization)"
      ],
      "metadata": {
        "id": "l9ASrB7uDcSc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "A = \"Time is an illusion. Lunchtime double so!\"\n",
        "A = re.sub(\"[^a-zA-Z]\",\" \",A)\n",
        "tokenized_A = [word for word in A.split()]\n",
        "tokenized_A"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EL7EQVzYDZER",
        "outputId": "0aeaf6ef-9abc-40fa-9024-84d60c64ec5e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Time', 'is', 'an', 'illusion', 'Lunchtime', 'double', 'so']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- nltk: 영어 corpus를 토큰화하기 위한 도구를 제공\n",
        "  - word_tokenize\n",
        "  - WordPunctTokenizer\n",
        "  - text_to_word_sequence (keras)"
      ],
      "metadata": {
        "id": "TAk1RgtfD33U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence"
      ],
      "metadata": {
        "id": "cJF2eQsYDZoO"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A = \"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\""
      ],
      "metadata": {
        "id": "wS6FYKlVDZuG"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1) word_tokenize"
      ],
      "metadata": {
        "id": "wCsrYkCfEo9l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "# nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iUI4RMEQEdbI",
        "outputId": "0d69188e-1cf0-4dc8-ac50-2e75a6dd41af"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'단어 토큰화1: {word_tokenize(A)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WIjlRUbeD5-j",
        "outputId": "197b2fb8-cad2-43a7-d737-d36b369d753d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어 토큰화1: ['Do', \"n't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr.', 'Jone', \"'s\", 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "word_tokenize결과\n",
        "- Don't -> Do, n't\n",
        "- Jone's -> Jone, 's"
      ],
      "metadata": {
        "id": "LpQ5qx_rE37a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2) WordPunctTokenizer"
      ],
      "metadata": {
        "id": "ZxolHVjHFC8Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'단어 토큰화2: {WordPunctTokenizer().tokenize(A)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71N3WVF5D6Fs",
        "outputId": "cc3676c6-3677-41d3-a70c-9cbd7780e47b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어 토큰화2: ['Don', \"'\", 't', 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr', '.', 'Jone', \"'\", 's', 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "WordPunctTokenizer결과\n",
        "- Don't -> Don, ', t (3개)\n",
        "- Jone's -> Jone, ',s (3개)"
      ],
      "metadata": {
        "id": "1K-0jUvvFStx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3) text_to_word_sequence"
      ],
      "metadata": {
        "id": "XObf4Xd3FnYj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'단어 토큰화3: {text_to_word_sequence(A)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05fD_aWTD6Jy",
        "outputId": "ffd39088-ddc5-4fef-a692-2621b69dc47b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어 토큰화3: [\"don't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', 'mr', \"jone's\", 'orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "text_to_word_sequence 결과\n",
        "- Don't -> don't\n",
        "- jone's -> jone's\n",
        "\n",
        "특징\n",
        "- 모든 알파벳을 소문자로 바꿈\n",
        "- 구두점(마침표, 컴마, 느낌표 등) 제거\n",
        "- 아포스트로피 보존"
      ],
      "metadata": {
        "id": "ryLt6wnxFvu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. 토큰화에서 고려해야 할 사항\n",
        "- 1) 구두점이나 특수 문자를 단순 제외해서는 안 된다.\n",
        "- 2) 줄임말과 단어 내에 띄어쓰기가 있는 경우, 단어를 하나로 인식할 수 있는 능력이 있어야 함.\n",
        "  - we're -> we are\n",
        "  - I'm -> I am"
      ],
      "metadata": {
        "id": "e8IKIx5NGEOU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 3) 표준 토큰화 예제\n",
        "- Penn Treebank Tokenization의 규칙\n",
        "  - 규칙 1. 하이푼(-)으로 구성된 단어는 하나로 유지한다.\n",
        "  - 규칙 2. doesn't와 같이 아포스트로피로 '접어'가 함께하는 단어는 분리해준다"
      ],
      "metadata": {
        "id": "EapPPYWRGbrn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"Starting a home-based restaurant may be an ideal. it doesn't have a food chain or restaurant of their own.\""
      ],
      "metadata": {
        "id": "WIp9bBobFqY_"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "\n",
        "tokenizer = TreebankWordTokenizer()\n",
        "\n",
        "print(f'트리뱅크 워드 토크나이저: {tokenizer.tokenize(sentence)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EV8NExAhFqc3",
        "outputId": "44d238d4-9bd1-4c87-812f-d2e3e424a8bb"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "트리뱅크 워드 토크나이저: ['Starting', 'a', 'home-based', 'restaurant', 'may', 'be', 'an', 'ideal.', 'it', 'does', \"n't\", 'have', 'a', 'food', 'chain', 'or', 'restaurant', 'of', 'their', 'own', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. 문장 토큰화\n",
        "- 문장 단위로 구분하는 작업. 문장 분류.\n",
        "- text 속 여러 문장들로부터 문장을 구분하기\n",
        "- 단순히 ?나 마침표,! 등으로 문장을 자르면 되지 않을까? 라고 생각할 수 있지만, 반드시 그렇지만은 않다.\n",
        "  - ex)IP 192.168.56.31 서버에 들어가서 로그 파일 저장해서 aaa@gmail.com로 결과 좀 보내줘. 그 후 점심 먹으러 가자.\n",
        "  - ex) Since I'm actively looking for Ph.D. students, I get the same question a dozen times every year."
      ],
      "metadata": {
        "id": "OAndOMhNG-I5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1) 영어\n",
        "- `from nltk.tokenize import sent_tokenize`"
      ],
      "metadata": {
        "id": "MszucRn3IQHp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "text1 = \"His barber kept his word. But keeping such a huge secret to himself was driving him crazy. Finally, the barber went up a mountain and almost to the edge of a cliff. He dug a hole in the midst of some reeds. He looked about, to make sure no one was near.\"\n",
        "\n",
        "print(f'문장 토큰화1: {sent_tokenize(text1)}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6d_lwOGmFqrB",
        "outputId": "b8968f97-6423-4ed4-fe0f-bf3533796cbe"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "문장 토큰화1: ['His barber kept his word.', 'But keeping such a huge secret to himself was driving him crazy.', 'Finally, the barber went up a mountain and almost to the edge of a cliff.', 'He dug a hole in the midst of some reeds.', 'He looked about, to make sure no one was near.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text2 = \"I am actively looking for Ph.D. students. and you are a Ph.D student.\"\n",
        "print('문장 토큰화2 :',sent_tokenize(text2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8M-geu6qD6Ou",
        "outputId": "2efa6cbe-2081-472c-b45d-aded696491a7"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "문장 토큰화2 : ['I am actively looking for Ph.D. students.', 'and you are a Ph.D student.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLTK는 단순히 마침표를 구분자로 하여 문장을 구분하지 않았기 때문에, Ph.D.를 문장 내의 단어로 인식하여 성공적으로 인식하는 것을 볼 수 있습니다"
      ],
      "metadata": {
        "id": "jdaEZw_dICY-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2) 한글\n",
        "- `kss`"
      ],
      "metadata": {
        "id": "uscZUE-kIUhX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install kss"
      ],
      "metadata": {
        "id": "NdRdHhN4IWPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import kss\n",
        "text = '딥 러닝 자연어 처리가 재미있기는 합니다. 그런데 문제는 영어보다 한국어로 할 때 너무 어렵습니다. 이제 해보면 알걸요?'\n",
        "\n",
        "print(f'한국어 문장 토큰화: {kss.split_sentences(text)}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VzFm6BcJIXWl",
        "outputId": "022f7774-18a9-41ac-9d41-e3817dd4d84e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Because there's no supported C++ morpheme analyzer, Kss will take pecab as a backend. :D\n",
            "For your information, Kss also supports mecab backend.\n",
            "We recommend you to install mecab or konlpy.tag.Mecab for faster execution of Kss.\n",
            "Please refer to following web sites for details:\n",
            "- mecab: https://github.com/hyunwoongko/python-mecab-kor\n",
            "- konlpy.tag.Mecab: https://konlpy.org/en/latest/api/konlpy.tag/#mecab-class\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "한국어 문장 토큰화: ['딥 러닝 자연어 처리가 재미있기는 합니다.', '그런데 문제는 영어보다 한국어로 할 때 너무 어렵습니다.', '이제 해보면 알걸요?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L8mUu4xDMCtC",
        "outputId": "aee889a9-a2be-4e7d-d7c1-7844cbbdb98b"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. 실습"
      ],
      "metadata": {
        "id": "BUx_OWo9MMBe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1) 영어"
      ],
      "metadata": {
        "id": "sFdQzy84MiPo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "\n",
        "text = \"I am actively looking for Ph.D. students. and you are a Ph.D. student.\"\n",
        "\n",
        "tokenized_text = word_tokenize(text)\n",
        "\n",
        "print(f'단어 토큰화: {tokenized_text}')\n",
        "print(f'품사 태깅: {pos_tag(tokenized_text)}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AHbG186VIXUy",
        "outputId": "fac0159d-e825-41ad-cbf7-4b44952b139c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어 토큰화: ['I', 'am', 'actively', 'looking', 'for', 'Ph.D.', 'students', '.', 'and', 'you', 'are', 'a', 'Ph.D.', 'student', '.']\n",
            "품사 태깅: [('I', 'PRP'), ('am', 'VBP'), ('actively', 'RB'), ('looking', 'VBG'), ('for', 'IN'), ('Ph.D.', 'NNP'), ('students', 'NNS'), ('.', '.'), ('and', 'CC'), ('you', 'PRP'), ('are', 'VBP'), ('a', 'DT'), ('Ph.D.', 'NNP'), ('student', 'NN'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " - PRP: 인칭 대명사\n",
        " - VBP: 동사\n",
        " - RB: 부사\n",
        " - VBG: 현재부사\n",
        " - IN: 전치사\n",
        " - NNP: 고유 명사\n",
        " - NNS: 복수형 명사\n",
        " - CC: 접속사\n",
        " - DT: 관사"
      ],
      "metadata": {
        "id": "ozq5WBosMQ0w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2) 한글\n",
        "- KoNLPy(코엔엘파이)의 형태소 분석기\n",
        "  - Okt(Open Korea Text)\n",
        "  - Mecab(메캅)\n",
        "  - Komoran(코모란)\n",
        "  - Hannanum(한나눔)\n",
        "  - Kkma(꼬꼬마)"
      ],
      "metadata": {
        "id": "MEP_3wp2MzgK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ① okt"
      ],
      "metadata": {
        "id": "3Nv0OQV6ObVN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install konlpy"
      ],
      "metadata": {
        "id": "58fSa7OPNcHj"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Okt\n",
        "from konlpy.tag import Kkma\n",
        "\n",
        "okt = Okt()\n",
        "kkma = Kkma()\n",
        "\n",
        "text = \"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"\n",
        "\n",
        "print(f'OKT 형태소 분석: {okt.morphs(text)}')\n",
        "print(f'OKT 품사 태깅: {okt.pos(text)}')\n",
        "print(f'OKT 명사 추출: {okt.nouns(text)}')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m9hblYUkIXTK",
        "outputId": "2b22311d-3fd5-4d44-9827-c28e9bd54397"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OKT 형태소 분석: ['열심히', '코딩', '한', '당신', ',', '연휴', '에는', '여행', '을', '가봐요']\n",
            "OKT 품사 태깅: [('열심히', 'Adverb'), ('코딩', 'Noun'), ('한', 'Josa'), ('당신', 'Noun'), (',', 'Punctuation'), ('연휴', 'Noun'), ('에는', 'Josa'), ('여행', 'Noun'), ('을', 'Josa'), ('가봐요', 'Verb')]\n",
            "OKT 명사 추출: ['코딩', '당신', '연휴', '여행']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ② Kkma"
      ],
      "metadata": {
        "id": "xvQo5PyeOeO1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'꼬꼬마 형태소 분석: {kkma.morphs(text)}')\n",
        "print(f'꼬꼬마 품사 태깅: {kkma.pos(text)}')\n",
        "print(f'꼬꼬마 명사 추출: {kkma.nouns(text)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yF80Mj35IXPJ",
        "outputId": "87b87000-5fa0-45e9-a8c9-aa0f4c679290"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "꼬꼬마 형태소 분석: ['열심히', '코딩', '하', 'ㄴ', '당신', ',', '연휴', '에', '는', '여행', '을', '가보', '아요']\n",
            "꼬꼬마 품사 태깅: [('열심히', 'MAG'), ('코딩', 'NNG'), ('하', 'XSV'), ('ㄴ', 'ETD'), ('당신', 'NP'), (',', 'SP'), ('연휴', 'NNG'), ('에', 'JKM'), ('는', 'JX'), ('여행', 'NNG'), ('을', 'JKO'), ('가보', 'VV'), ('아요', 'EFN')]\n",
            "꼬꼬마 명사 추출: ['코딩', '당신', '연휴', '여행']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s3BXSEsLIXLw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ggC_q0goIXJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qK_vntEjIXF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bapyVkXaIXBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "clukfte9IW_k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}