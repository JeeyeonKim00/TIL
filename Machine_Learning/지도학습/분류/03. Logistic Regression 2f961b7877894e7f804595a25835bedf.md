# Logistic Regression

- **로지스틱 회귀:**
    - 선형회귀 방식을 분류에 적용한 알고리즘 → 분류에 사용됨
    - 선형회귀 계열
    - 함수를 통해 선형함수의 회귀 최적선을 찾는 것이 아니라 시그모이드(Sigmoid)함수 최적선을 찾고 이 시그모이드 함수의 반환 값을 확률로 간주해 확률에 따라 분류를 결정하는 것.
    - 선형회귀의 방식을 기반으로 하되, 시그모이드 함수를 이용해 ‘분류’ 를 수행하는 회귀
    - 선형회귀와 유사한데 출력값이 0~1 사이의 값으로 제한되도록 변형된 모델.
    
    - Logistic Regression 설명1
        
        Logistic Regression 모델은 선형 회귀(Linear Regression) 모델과 비슷한 형태를 가지며, 입력 데이터의 가중합을 **시그모이드 함수(sigmoid function)**에 적용하여 0과 1 사이의 값으로 변환합니다. 시그모이드 함수는 입력값이 무한대로 커지면 1에 수렴하고, 무한대로 작아지면 0에 수렴하는 함수입니다. 따라서, Logistic Regression 모델은 입력값을 시그모이드 함수에 적용하여 0.5보다 크면 1, 0.5보다 작으면 0으로 분류합니다.
        
        Logistic Regression 모델은 모델의 복잡도를 조절할 수 있는 하이퍼파라미터인 **규제 파라미터(C)**를 사용합니다. 규제 파라미터는 모델이 과적합(overfitting)되지 않도록 조절하는 역할을 합니다. L1 규제와 L2 규제를 모두 지원하며, 각 규제 방법은 모델의 가중치(weight)를 어떤 방향으로 조절할지를 결정합니다. L1 규제는 가중치의 절대값에 비례하는 패널티를 적용하여 불필요한 특성(feature)을 제거하는 효과가 있으며, L2 규제는 가중치의 제곱에 비례하는 패널티를 적용하여 가중치가 작아지도록 하는 효과가 있습니다.
        
        Logistic Regression 모델은 이진 분류 문제에서 강력한 성능을 발휘하며, 특히 선형 분류 문제에서 좋은 성능을 보입니다. 또한, 모델이 학습한 가중치를 해석하여 어떤 특성이 분류 결정에 영향을 미치는지 파악할 수 있습니다. 하지만, 복잡한 비선형 분류 문제에서는 성능이 떨어지는 경우가 많습니다.
        
    - LR 설명2
        
        Logistic Regression은 선형 회귀와 유사한데, **출력값이 0과 1 사이**의 값으로 제한되도록 변형된 모델입니다. 이를 위해 입력 데이터의 **가중합(가중치와 특성값의 곱의 합)**을 **시그모이드 함수**(sigmoid function)에 적용합니다. 시그모이드 함수는 S자 형태의 곡선으로, 입력값이 큰 음수일 때 0에 가까운 값, 입력값이 큰 양수일 때 1에 가까운 값, 입력값이 0일 때 0.5의 값을 출력합니다.
        
        이렇게 입력 데이터의 **가중합을 시그모이드 함수에 적용하면, 모델의 출력값은 0과 1 사이의 값으로 제한됩니다.** 이 값은 해당 샘플이 양성 클래스에 속할 확률로 해석할 수 있습니다. 예를 들어, 출력값이 0.7일 경우 해당 샘플이 양성 클래스에 속할 확률이 70%라는 것을 의미합니다. 이렇게 변환된 출력값을 사용해 분류를 수행합니다. 일반적으로 0.5를 기준으로 출력값이 0.5 이상인 경우 양성 클래스에 속한다고 판단하고, 이하인 경우 음성 클래스에 속한다고 판단합니다.
        
        **이때 가중치는 주어진 데이터를 가장 잘 설명하는 최적의 값으로 학습됩니다.** 이 과정에서 로지스틱 회귀는 **비용 함수(cost function)**를 사용하여 예측값과 실제값 사이의 차이를 최소화하도록 가중치를 조정합니다. 보통 로지스틱 회귀에서는 최적화 알고리즘인 경사 하강법(gradient descent)을 사용하여 비용 함수를 최소화합니다.
        
    - 비용함수, 경사하강 설명
        
        비용함수와 경사하강법은 주로 분류 문제에서 사용되는 알고리즘인 로지스틱 회귀에서 사용됩니다. 하지만 선형 회귀에서도 비용함수와 경사하강법을 사용할 수 있습니다. 선형 회귀에서는 보통 평균제곱오차(Mean Squared Error, MSE)를 비용함수로 사용하고, 이를 최소화하는 방향으로 경사하강법을 적용합니다. 따라서 비용함수와 경사하강법은 회귀 문제에서도 중요한 개념입니다.
        
    
- 파라미터
1. **penalty:** 규제(regularization) 유형을 결정하는 하이퍼파라미터입니다. 
    1. L1
    2. L2
    3. ElasticNet
2. **C:** 규제 강도를 결정하는 하이퍼파라미터입니다. C 값이 작을수록 규제가 강해지며, C 값이 클수록 규제가 약해집니다. (1/alpha)
3. **solver:** 최적화 알고리즘을 지정하는 하이퍼파라미터입니다. 
    1. **liblinear :** 국소 최적화 이슈, 병렬 최적화 X → L1, L2 모두 가능
    2. newton-cg: 종 더 정교한 최적화, 대용량의 데이터에선 속도가 많이 느려짐 → L2 만 가능
    3. lbfgs: sovler의 기본설정 값, 메모리공간 절약, CPU 코어 수가 많다면 최적화 병렬 수행 → L2 만 가능
    4. sag: 경사하강법 기반의 최적화 → L2 만 가능
    5. saga: sag와 유사, L1정규화 가능케 함 → L1, L2 모두 가능

# 데이터 불러오기

```python
import pandas as pd
import matplotlib.pyplot as plt
%matplotlib inline

from sklearn.datasets import load_breast_cancer

cancer = laod_breast_cancer()
```

# 데이터 전처리

선형모델을 사용할 때는 **표준화**를 진행하는 것이 좋다.

```python
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

scaler = StandardScaler()
data_scaled = scaler.fit_transform(cancer.data) # w전처리

X_train, X_test, y_train, y_test = train_test_split(
			data_scaled, cancer.target, test_size=0.3, random_state=0)
```

```python
**from sklearn.linear_model import LogisticRegression**

lr_clf = LogisticRegression() # solver= lbfgs(기본값)
lr_clf.fit(X_train, y_train)
lr_pred = lr_clf.predict(X_test)
```

# 평가

```python
from sklearn.metrics import accuracy_score, roc_auc_score

print('Accuracy: {:.3f}, ROC_AUC: {:.3f}'.format(accuracy_score(lr_pred,y_test),roc_auc_score(lr_pred, y_test)))

>>>**Accuracy: 0.977, ROC_AUC: 0.978**
```

# solver의 변화에 따른 성능 측정

```python
solvers = ['lbfgs','liblinear','newton-cg','sag','saga']

for solver in solvers:
		lr_clf = LogisticRegression(solver=solver, max_iter=600)
		lr_clf.fit(X_train, y_train)

		lr_pred = lr_clf.predict(X_test)

		print(solver)
    print('Accuracy: {:.3f}, ROC_AUC: {:.3f}'.format(accuracy_score(lr_pred,y_test),roc_auc_score(lr_pred, y_test)))
    print()

>>>>>
**lbfgs**
Accuracy: 0.977, ROC_AUC: 0.978

**liblinear
Accuracy: 0.982, ROC_AUC: 0.983**

**newton-cg**
Accuracy: 0.977, ROC_AUC: 0.978

**sag
Accuracy: 0.982, ROC_AUC: 0.983**

**saga
Accuracy: 0.982, ROC_AUC: 0.983**
```

# GridSearchCV 로 최적화

```python
from sklearn.model_selection import GridSearchCV

params={'solver':['liblinear', 'lbfgs'],
        'penalty':['l2', 'l1'],
        'C':[0.01, 0.1, 1, 1, 5, 10]}

lr_clf = LogisticRegression()
grid_clf = GridSearchCV(lr_clf, param_grid = params, scoring='accuray','cv=3)

grid_clf.fit(data_scaled, cancer.target)
```

```python
print('''최적 하이퍼 파라미터:{}, 
최적 평균 정확도:{:.3f}'''.format(grid_clf.best_params_, 
                                                  grid_clf.best_score_))

>>> 
최적 하이퍼 파라미터:{'C': 0.1, 'penalty': 'l2', 'solver': '**liblinear**'}, 
최적 평균 정확도:0.979
```